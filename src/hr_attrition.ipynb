{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## **_Enterprise Data Science and Analytics - Enterprise Data Science Bootcamp_**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  ### **HR Attrition Project - EDSB25_26**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Ana Rita Martins 20240821\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Joana Coelho 2024080\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Pedro Fernandes 20240823\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Ricardo Silva 20240824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data Science and Analytics are reshaping how organizations solve problems across diverse industries. Through systematic data analysis and predictive modeling, evidence-based solutions can be developed, enabling more reliable decision-making and greater efficiency.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    In Human Resources, predictive analytics supports critical functions such as employee retention, workforce planning, and automated CV screening.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    This project focuses on developing predictive models to assess the likelihood of employee resignation. By analyzing factors ranging from demographics to job satisfaction, the models aim to provide interpretable insights that highlight key drivers of attrition. These insights will help HR leaders take proactive steps to reduce turnover and retain talent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 1. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from summarytools import dfSummary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from itertools import product\n",
    "import optuna\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.base import clone\n",
    "import shap\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## 2. Importing Data and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/raw/HR_Attrition_Dataset.csv')\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) \n",
    "data.describe() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From this initial inspection what immediately stands out is that we have 3 constant features: \"EmployeeCount\", \"StandardHours\", and \"Over18\". We can remove those straight away. Additionally, the employee number (ID) feature, does not seem to contain any relevant info, and  we'll drop it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['EmployeeCount','Over18','StandardHours','EmployeeNumber'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = data.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "for col in cat_cols: \n",
    "    print(f\"Value counts for column '{col}':\")\n",
    "    print(data[col].value_counts())\n",
    "    print(\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSummary(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From the summary above, we verified that the data set doesn't contain duplicates, and we also gathered information about the data's distribution and main statistics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    What we can note is that, beasides our target, we have a couple of other binary features. Let's encode those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Attrition'] = data['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "data['Gender'] = data['Gender'].map({'Male': 1, 'Female': 0})\n",
    "data['OverTime'] = data['OverTime'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Let's now have a look at how the distribution of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=data['Attrition'], hue=data['Attrition'], legend=False)\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.title('Distribution of the Target Variable (Attrition)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We can observe that our target cariable is quite imbalanced. This will require extra attention in later steps, namely when splitting the dataset into train, validation and test sets, as well as during the modelling stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # **3. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We'll start by plotting histograms to visually assess the distribution of the numeric features; this will allows us to spot any relevant patterns or trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(20, 15))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The histograms reveal some important patterns in the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Once again we can observe that the **target variable** is highly skewed toward staying in the company.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Concerning demographics, **age** follows an approximately bell-shaped distribution, centered around 30-40; **Gender** is skewed with more males than females.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Features that are related to **work characteristics** (YearsAtCompany, TotalWorkingYears, YearsInCurrentRole, Overtime) are right-skewed, indicating many relatively new employees and fewer with long careers; working overtime is not common.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - **Income**: Salaries and rates are right-skewed, with few very high earners.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - **Satisfaction-related** variables are discrete and somewhat skewed toward higher ratings, while PerformanceRating shows very little variation (nearly all at level 3), suggesting limited predictive value.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Overall, the data displays strong imbalance and skewness patterns that will require careful consideration during modeling, suggesting it could benefit from stratified splits, and algorithms robust to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting  numerical columns (binaries excluded)\n",
    "binary_cols = ['Attrition', 'Gender', 'OverTime']\n",
    "num_cols = [col for col in data.select_dtypes(include=['int64', 'float64']).columns if col not in binary_cols]\n",
    "\n",
    "# Boxplots for each numerical feature\n",
    "n_cols = 5\n",
    "n_rows = -(-len(num_cols) // n_cols)  \n",
    "\n",
    "plt.figure(figsize=(20, 4*n_rows))\n",
    "\n",
    "for i, col in enumerate(num_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.boxplot(y=data[col])\n",
    "    plt.title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The boxplots highlight the extent of skewness and make the outliers stand out clearly, which complements the histogram analysis above.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Outliers are especially relevant in income and employment duration related-variables, which may need special handling. We'll decide how to handle them further down.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - For demographic/job characteristics (Age, DistanceFromHome, JobLevel, Education) featured the distributions are fairly compact with few outliers, aligning with the unimodal/bell-like shapes seen in histograms.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Ordinal satisfaction and variables show limited spread, consistent with their discrete scale, with some level of skew toward higher values. Their limited range may reduce their explanatory power.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - PerformanceRating shows very little variation (nearly all values at level 3) confirming its limited usefulness as a predictive feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Subsequent steps may differ based on the category of each feature. Therefore, we’ll create lists that group feature names by their respective types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly define groups that cannot be inferred reliably\n",
    "feature_groups = {\n",
    "    \"binary\": ['Gender', 'OverTime'],\n",
    "    \"ordinal\": [\n",
    "        'Education','EnvironmentSatisfaction','JobInvolvement',\n",
    "        'JobLevel','JobSatisfaction','PerformanceRating',\n",
    "        'RelationshipSatisfaction','StockOptionLevel','WorkLifeBalance'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Infer remaining types\n",
    "all_features = data.columns.drop('Attrition')\n",
    "\n",
    "# Categorical = object dtype except those explicitly listed\n",
    "explicit_non_continuous = feature_groups[\"binary\"] + feature_groups[\"ordinal\"]\n",
    "categorical = (\n",
    "    data.select_dtypes(include='object')\n",
    "        .columns.difference(explicit_non_continuous)\n",
    "        .tolist()\n",
    ")\n",
    "\n",
    "# Continuous = numeric except explicit lists\n",
    "continuous = (\n",
    "    all_features\n",
    "        .difference(categorical + explicit_non_continuous)\n",
    "        .tolist()\n",
    ")\n",
    "\n",
    "feature_groups[\"categorical\"] = categorical\n",
    "feature_groups[\"continuous\"] = continuous\n",
    "feature_groups['non-continuous'] = feature_groups['binary'] + feature_groups['ordinal'] + categorical\n",
    "\n",
    "feature_groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Let's now look at the distribution of our non-continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in feature_groups['non-continuous']:\n",
    "\n",
    "    ax = sns.countplot(y=data[feature],order=data[feature].value_counts(ascending=False).index)\n",
    "    ax.set_xlabel('Number of Employees')\n",
    "\n",
    "    # Get data label values and concatenate them\n",
    "    abs_values = data[feature].value_counts(ascending=False).values\n",
    "    rel_values = data[feature].value_counts(ascending=False, normalize=True).values * 100\n",
    "    data_labels = [f'{label[0]} ({label[1]:.1f}%)' for label in zip(abs_values, rel_values)]\n",
    "\n",
    "    ax.bar_label(container=ax.containers[0], labels=data_labels)\n",
    "    ax.margins(x=0.25)\n",
    "    \n",
    " \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    From the variables that, a priori, we'd think could be related with attrition, we find that:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - roughly 30% of employees work overtime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - roughly 40% have low to medium levels of satisfaction with the work environment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - roughly 30% report low to medium levels of job involvement\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - nearly 40% report low to medium job satisfaction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - another nearly 40% have low to medium levels of satisfaction with relationships at work\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - and about 5% report bad work-life balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To better understand what might be contributing to employees’ decisions to quit, we'll next plot the non-continuous features against the target variable. We’ll also measure the attrition rate within each category. This will show us whether some groups are more prone to leaving than others, irrespective of their overall frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in feature_groups['non-continuous']:\n",
    "\n",
    "    # Get within category proportions\n",
    "    proportions = data.groupby(feature)['Attrition'].value_counts(normalize=True)\n",
    "\n",
    "    # Plot\n",
    "    ax = sns.countplot(y=data[feature], hue=data['Attrition'], order=data[feature].value_counts().sort_index().index)\n",
    "    ax.set_xlabel('Number of Employees')\n",
    "\n",
    "    # Insert proportions as data labels\n",
    "    for i, container in enumerate(ax.containers):\n",
    "        labels = [f'{proportions.loc[d,i]:.1%}' for d in sorted(data[feature].unique())]\n",
    "        ax.bar_label(container, labels)\n",
    "\n",
    "    ax.margins(x=0.15)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the plots above we find the following trends:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Department-level & Job roles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Sales and Human Resources show a higher proportion of employees quitting compared to R&D.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Within job roles, HR professionals tend to leave more often, but so do Lab Technicians, even though they are part of the R&D department.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Sales Representatives have the highest attrition rate across all job roles, whereas higher-level roles—such as managers and directors—show very low attrition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Personal characteristics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Single employees appear more likely to quit.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Work conditions and workload\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Employees who work overtime, travel frequently, or have poor work–life balance are more likely to leave.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Low satisfaction with the work environment, job involvement, overall job satisfaction, and relationships at work is also strongly associated with higher attrition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Job level and hierarchy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Employees in lower hierarchical levels tend to leave more often. However, attrition proportions do not strictly follow the hierarchical ranking order.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Stock ownership\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Employees with no stock options (stock option level 0) are more prone to quitting. This is not surprising, as offering stock is a common strategy to increase engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's now run an equivalent analysis with our continuous features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " We'll plot both their probability density function and violin plots and assess how their distribution relates to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure Attrition is binary for plotting aesthetics\n",
    "df_plot = data.copy()\n",
    "df_plot[\"Attrition\"] = df_plot[\"Attrition\"].astype(str)\n",
    "\n",
    "#attrition_num = df_plot[\"Attrition\"]\n",
    "\n",
    "continuous_vars = feature_groups[\"continuous\"]\n",
    "\n",
    "def plot_kde_violin(df, col):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # KDE plot\n",
    "    sns.kdeplot(\n",
    "        data=df, x=col, hue=\"Attrition\",\n",
    "        common_norm=False, fill=True, alpha=0.4, ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(f\"KDE of {col} by Attrition\")\n",
    "    axes[0].set_xlabel(col)\n",
    "    axes[0].set_ylabel(\"Density\")\n",
    "    \n",
    "    # Violin plot\n",
    "    sns.violinplot(\n",
    "        data=df, hue=\"Attrition\", y=col,\n",
    "        inner=\"box\", ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(f\"Violin Plot of {col} by Attrition\")\n",
    "    axes[1].set_xlabel(\"Attrition\")\n",
    "    axes[1].set_ylabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate combined plots for all continuous variables\n",
    "for col in continuous_vars:\n",
    "    plot_kde_violin(df_plot, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Some features show noticeable differences in their distributions depending on whether the employee quit or stayed.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Age and career stage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Employees who quit tend to be younger.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - This aligns with lower values observed in Total Working Years, Years at Company, Years in Current Role, and Years with Current Manager.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Early-career employees may be more inclined to change jobs or roles, contributing to these lower tenure metrics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Compensation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - Monthly income appears influential: employees with lower income are more likely to leave, which is expected. The same applies to daily rate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Distance from home\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - The larger the distance from home to work, the more likely the employees are to leave.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Other features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - The remaining continuous features either show similar distributions across attrition groups or differences too small to be clearly meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We’ll now take look at the correlations among the features, including the target variable. This will help us identify potential collinearity, as well as highlight which features are associated with attrition. Since several features are not strictly numeric or continuous, we’ll use Spearman’s correlation, which measures monotonic relationships by correlating feature ranks rather than their raw values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We'll exclude strictly nominal categorical variables (like Gender, Department, JobRole) because Spearman is rank-based, not meant for unordered categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting valid variables for Spearman\n",
    "\n",
    "df_corr = data.copy()\n",
    "\n",
    "ordinal_features = feature_groups[\"ordinal\"]\n",
    "continuous_features = feature_groups[\"continuous\"]\n",
    "\n",
    "spearman_vars = continuous_features + ordinal_features + [\"Attrition\"]\n",
    "df_spearman = df_corr[spearman_vars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Spearman correlation matrix\n",
    "\n",
    "spearman_matrix = df_spearman.corr(method=\"spearman\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting sorted correlations with Attrition\n",
    "\n",
    "attrition_corr = spearman_matrix[\"Attrition\"].drop(\"Attrition\")\n",
    "attrition_corr_sorted = attrition_corr.sort_values(ascending=False)\n",
    "\n",
    "print(attrition_corr_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Spearman correlation matrix\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(spearman_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(spearman_matrix, mask=mask, cmap=\"coolwarm\", center=0, annot = True, fmt=\".2f\", square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title(\"Spearman Correlation Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the analyses and visualization above we observe that:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - YearsAtCompany, YearsInCurrentRole, YearsWithCurrManager, TotalWorkingYears, JobLevel, MonthlyIncome, StockOptionLevel and Age are the strongest monotonic predictors of Attrition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " These are indicators that relate to tenure, seniority, and stability and they're in agreement with HR domain knowledge: attrition is highest among newer, younger, lower-level employees.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - JobSatisfaction, JobInvolvement, EnvironmentSatisfaction Tshow mild but potentially meaningful associations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Employees with lower satisfaction or lower involvement show slightly higher attrition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = attrition_corr_sorted.abs().sort_values(ascending=False).head(12).index\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(spearman_matrix.loc[top_features, top_features], dtype=bool))\n",
    "sns.heatmap(spearman_matrix.loc[top_features, top_features], mask=mask,\n",
    "            cmap=\"coolwarm\", center=0, annot=True, fmt=\".2f\")\n",
    "plt.title(\"Top Spearman Correlated Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The heatmap shows that several of the variables most strongly correlated with attrition are also highly collinear with each other. In particular, the following groups demonstrate very strong monotonic relationships (ρ > 0.70):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - JobLevel — MonthlyIncome (ρ ≈ 0.92)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - YearsInCurrentRole — YearsWithCurrManager (ρ ≈ 0.85)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - TotalWorkingYears — MonthlyIncome (ρ ≈ 0.71)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " These features are all measures of: Tenure, Seniority, Career progression, Employee stability, which explains why they are tightly correlated with each other and with lower attrition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " While colinearity doesn't harm tree-based models, it does affect linear models like linear regression. Besides, it It also leads to unnecessary redundancy in the feature set. Keeping all of them increases the demand for computational powerr and increases the risk of overfitting. By the end of our feature selection process, we should aim to keep at most 2 or 3 representative variables of this set. And for regression models, we'll explicitly remove correlated pairs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Another way to circumvent colinearity is to combine several colinear raw variables into a single engineered feature. Let's do that below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before any encoding and feature selection steps we'll start by defining x and y, and defining the train–test split. Doing this at this stage is critical to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.drop('Attrition', axis=1).copy()\n",
    "y = data['Attrition'].copy()\n",
    "\n",
    "# Train–test split (20% test, stratified by target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,      # keep same attrition proportion in train/test; very important given class imbalance\n",
    "    shuffle=True     \n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TenureIndex (Average of three tenure-related variables)\n",
    "\n",
    "data[\"TenureIndex\"] = (\n",
    "    data[\"YearsAtCompany\"] +\n",
    "    data[\"YearsInCurrentRole\"] +\n",
    "    data[\"YearsWithCurrManager\"]\n",
    ") / 3\n",
    "\n",
    "\n",
    "# PromotionGap (Time in company since last promotion: a proxy for stagnation)\n",
    "\n",
    "data[\"PromotionGap\"] = data[\"YearsAtCompany\"] - data[\"YearsSinceLastPromotion\"]\n",
    "\n",
    "# avoid negative values if any weird records exist\n",
    "data[\"PromotionGap\"] = data[\"PromotionGap\"].clip(lower=0)\n",
    "\n",
    "# EngagementIndex (Composite of satisfaction / involvement metrics)\n",
    "\n",
    "engagement_cols = [\n",
    "    \"JobInvolvement\",\n",
    "    \"JobSatisfaction\",\n",
    "    \"EnvironmentSatisfaction\",\n",
    "    \"RelationshipSatisfaction\"\n",
    "]\n",
    "\n",
    "data[\"EngagementIndex\"] = data[engagement_cols].mean(axis=1)\n",
    "\n",
    "\n",
    "# IncomeVsRoleMedian (MonthlyIncome relative to median for JobRole)\n",
    "\n",
    "# Compute medians by JobRole on TRAIN ONLY\n",
    "role_medians = X_train.groupby(\"JobRole\")[\"MonthlyIncome\"].median()\n",
    "\n",
    "# Map to train and test\n",
    "X_train[\"IncomeVsRoleMedian\"] = (\n",
    "    X_train[\"MonthlyIncome\"] / X_train[\"JobRole\"].map(role_medians)\n",
    ")\n",
    "\n",
    "X_test[\"IncomeVsRoleMedian\"] = (\n",
    "    X_test[\"MonthlyIncome\"] / X_test[\"JobRole\"].map(role_medians)\n",
    ")\n",
    "\n",
    "data['Income_Rate_Ratio'] = data['MonthlyIncome'] / data['MonthlyRate']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered = [\"TenureIndex\", \"PromotionGap\", \"EngagementIndex\", \"IncomeVsRoleMedian\", \"Income_Rate_Ratio\"]\n",
    "\n",
    "spearman_corrs = (\n",
    "    data[engineered + [\"Attrition\"]]\n",
    "    .corr(method=\"spearman\")[\"Attrition\"]\n",
    "    .drop(\"Attrition\")\n",
    ")\n",
    "\n",
    "print(spearman_corrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Rebuilding feature groups on X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Categorical features inferred from dtype 'object'\n",
    "categorical_features = list(\n",
    "    X_train.select_dtypes(include='object').columns.drop(['BusinessTravel'])\n",
    ")\n",
    "\n",
    "# 2. Binary features (defined them manually above)\n",
    "binary_features = ['Gender', 'OverTime']\n",
    "\n",
    "# 3. Ordinal features (predefined list above)\n",
    "ordinal_features = [\n",
    "    'BusinessTravel','Education','EnvironmentSatisfaction','JobInvolvement',\n",
    "    'JobLevel','JobSatisfaction','PerformanceRating',\n",
    "    'RelationshipSatisfaction','StockOptionLevel','WorkLifeBalance'\n",
    "]\n",
    "\n",
    "# 4. Non-continuous = categorical + binary + ordinal\n",
    "non_continuous_features = categorical_features + binary_features + ordinal_features\n",
    "\n",
    "# 5. Continuous = everything else except the target (including engineered features)\n",
    "continuous_features = list(\n",
    "    X_train.columns.difference(non_continuous_features)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical nominal:\", categorical_features)\n",
    "print(\"Binary:\", binary_features)\n",
    "print(\"Ordinal:\", ordinal_features)\n",
    "print(\"Continuous (incl. engineered):\", continuous_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Defining the preprocessing (encoders + passthrough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# --- Split ordinal features: BusinessTravel vs the rest ---\n",
    "ordinal_bt = [\"BusinessTravel\"]\n",
    "ordinal_other = [f for f in ordinal_features if f != \"BusinessTravel\"]\n",
    "\n",
    "# 1) Ordinal encoder for BusinessTravel with meaningful order\n",
    "bt_categories = [[\"Non-Travel\", \"Travel_Rarely\", \"Travel_Frequently\"]]\n",
    "\n",
    "bt_ordinal_transformer = OrdinalEncoder(\n",
    "    categories=bt_categories,\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1,\n",
    ")\n",
    "\n",
    "# 2) Ordinal encoder for all other ordinal features (numeric scales 1–4/5 etc.)\n",
    "other_ordinal_transformer = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1,\n",
    ")\n",
    "\n",
    "# 3) One-hot encoder for nominal + binary\n",
    "onehot_transformer = OneHotEncoder(\n",
    "    drop=None,\n",
    "    handle_unknown=\"ignore\",\n",
    "    sparse_output=False,\n",
    ")\n",
    "\n",
    "# 4) ColumnTransformer tying everything together\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # BusinessTravel with explicit order: Non-Travel < Rarely < Frequently\n",
    "        (\"ord_bt\", bt_ordinal_transformer, ordinal_bt),\n",
    "\n",
    "        # Other ordinal features (Education, JobSatisfaction, etc.)\n",
    "        (\"ord\", other_ordinal_transformer, ordinal_other),\n",
    "\n",
    "        # Nominal + binary features\n",
    "        (\"nom\", onehot_transformer, categorical_features + binary_features),\n",
    "\n",
    "        # Continuous (including engineered ones like TenureIndex, IncomeVsRoleMedian)\n",
    "        (\"num\", \"passthrough\", continuous_features),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When preprocess.fit(X_train) is called, it learns: category mappings for ordinal features; dummy columns for nominal + binary.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " Then preprocess.transform(...) will apply this same mapping to train & test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Combining preprocessing + scaling into a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_preprocess = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('scale', StandardScaler())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Fitting preprocessing only on training data and transform both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training data only (no leakage)\n",
    "pipeline_preprocess.fit(X_train)\n",
    "\n",
    "# Transform train and test with the same fitted pipeline\n",
    "X_train_processed = pipeline_preprocess.transform(X_train)\n",
    "X_test_processed  = pipeline_preprocess.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_processed shape:\", X_train_processed.shape)\n",
    "print(\"X_test_processed shape:\",  X_test_processed.shape)\n",
    "\n",
    "#print(\"Number of feature names:\", len(feature_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Grab the ColumnTransformer from the pipeline\n",
    "ct = pipeline_preprocess.named_steps['preprocess']\n",
    "\n",
    "# 2. Ask it for the output feature names\n",
    "feature_names = ct.get_feature_names_out()\n",
    "\n",
    "# 3. Now rebuild the DataFrames\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names, index=X_train.index)\n",
    "X_test_df  = pd.DataFrame(X_test_processed,  columns=feature_names, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Chi-square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_for_feature(X_col, y):\n",
    "    \"\"\"Return chi2 and p-value for one categorical feature vs target.\"\"\"\n",
    "    table_observed = pd.crosstab(y, X_col)\n",
    "    chi2, pvalue, dof, expected = stats.chi2_contingency(table_observed.values)\n",
    "    return chi2, pvalue\n",
    "\n",
    "\n",
    "def chi_square_for_features(X_train, y_train, alpha=0.05):\n",
    "    \"\"\"Run chi-square for each column in X_train and return a summary DataFrame.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for var in X_train.columns:\n",
    "        chi2, pvalue = chi_square_for_feature(X_train[var], y_train)\n",
    "        results.append({\n",
    "            \"feature\": var,\n",
    "            \"chi2\": chi2,\n",
    "            \"p_value\": pvalue,\n",
    "            \"significant\": pvalue < alpha\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Sort by p-value (smallest p-value = strongest evidence of association)\n",
    "    results_df = results_df.sort_values(\"p_value\")\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_results = chi_square_for_features(\n",
    "    X_train[non_continuous_features],\n",
    "    y_train,\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "chi2_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_information_from_ct(ct, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Compute mutual information between each encoded feature and the target,\n",
    "    using a fitted ColumnTransformer 'ct' (without scaling).\n",
    "    Returns a sorted DataFrame with MI scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Transform X_train with the fitted ColumnTransformer (no scaling)\n",
    "    X_train_enc = ct.transform(X_train)\n",
    "    feature_names = ct.get_feature_names_out()\n",
    "\n",
    "    X_train_enc_df = pd.DataFrame(\n",
    "        X_train_enc,\n",
    "        columns=feature_names,\n",
    "        index=X_train.index\n",
    "    )\n",
    "\n",
    "    # 2. Build a mask of which features are discrete\n",
    "    col_series = X_train_enc_df.columns.to_series()\n",
    "    discrete_mask = col_series.str.startswith(('ord__', 'nom__'))\n",
    "\n",
    "    # 3. Compute mutual information\n",
    "    mi_scores = mutual_info_classif(\n",
    "        X_train_enc_df,\n",
    "        y_train,\n",
    "        discrete_features=discrete_mask.values,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 4. Build results DataFrame\n",
    "    mi_df = pd.DataFrame({\n",
    "        'Feature': X_train_enc_df.columns,\n",
    "        'MI': mi_scores,\n",
    "        'Discrete': discrete_mask.values\n",
    "    })\n",
    "\n",
    "    mi_df.sort_values('MI', ascending=False, inplace=True)\n",
    "    mi_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return mi_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fitted ColumnTransformer from the pipeline\n",
    "ct = pipeline_preprocess.named_steps['preprocess']\n",
    "\n",
    "mi_results = compute_mutual_information_from_ct(ct, X_train, y_train)\n",
    "display(mi_results.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## L1 Logistic Regression (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_with_l1_logistic(X_train_df, y_train, C=1.0):\n",
    "    \"\"\"\n",
    "    Run L1-penalized Logistic Regression to select features.\n",
    "    Returns a DataFrame with coefficients and selection mask.\n",
    "    \"\"\"\n",
    "    # L1 logistic regression with class balancing (important for attrition)\n",
    "    l1_model = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        class_weight='balanced',\n",
    "        C=C,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    selector = SelectFromModel(l1_model, prefit=False)\n",
    "    selector.fit(X_train_df, y_train)\n",
    "\n",
    "    # Selected mask\n",
    "    mask = selector.get_support()\n",
    "\n",
    "    # Coefficients\n",
    "    coefs = selector.estimator_.coef_[0]\n",
    "\n",
    "    # Build results table\n",
    "    results = pd.DataFrame({\n",
    "        \"Feature\": X_train_df.columns,\n",
    "        \"Coefficient\": coefs,\n",
    "        \"Selected\": mask\n",
    "    })\n",
    "\n",
    "    # Absolute magnitude for sorting\n",
    "    results[\"AbsCoef\"] = results[\"Coefficient\"].abs()\n",
    "    results = results.sort_values(\"AbsCoef\", ascending=False)\n",
    "\n",
    "    return results, mask\n",
    "\n",
    "l1_results, l1_mask = select_with_l1_logistic(X_train_df, y_train)\n",
    "display(l1_results.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Features with Selected = True are part of the sparse LASSO-selected subset. Larger coefficients (in magnitude) reflect stronger linear effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Random Forest captures: nonlinearities, interactions, categorical effects, monotonic or non-monotonic patterns. Works very well alongside LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_with_random_forest(X_train_df, y_train, n_estimators=500):\n",
    "    \"\"\"\n",
    "    Train a Random Forest and return a DataFrame with feature importances.\n",
    "    \"\"\"\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=42,\n",
    "        class_weight='balanced',\n",
    "        max_depth=None,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    rf.fit(X_train_df, y_train)\n",
    "\n",
    "    importances = rf.feature_importances_\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        \"Feature\": X_train_df.columns,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "    return results, rf\n",
    "\n",
    "rf_results, rf_model = select_with_random_forest(X_train_df, y_train)\n",
    "display(rf_results.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## XGBoost Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " XGBoost is often very strong at discovering: threshold effects, feature interactions, nonlinear jump patterns, sparse informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_with_xgboost(X_train_df, y_train):\n",
    "    \"\"\"\n",
    "    Train XGBoost and return feature importances.\n",
    "    \"\"\"\n",
    "\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "    xgb.fit(X_train_df, y_train)\n",
    "\n",
    "    importances = xgb.feature_importances_\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        \"Feature\": X_train_df.columns,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "    return results, xgb\n",
    "\n",
    "xgb_results, xgb_model = select_with_xgboost(X_train_df, y_train)\n",
    "display(xgb_results.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Table Combining Feature Selection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting from all encoded features\n",
    "\n",
    "# Base table: one row per encoded feature\n",
    "unified_fs = pd.DataFrame({\n",
    "    \"Feature\": X_train_df.columns\n",
    "})\n",
    "\n",
    "\n",
    "# Merging MI results\n",
    "\n",
    "# Keep only needed columns from MI\n",
    "mi_short = mi_results[[\"Feature\", \"MI\"]]\n",
    "\n",
    "unified_fs = unified_fs.merge(\n",
    "    mi_short,\n",
    "    on=\"Feature\",\n",
    "    how=\"left\")\n",
    "\n",
    "#Merging L1 Logistic Regression results\n",
    "\n",
    "l1_short = l1_results[[\"Feature\", \"Coefficient\", \"Selected\"]]\n",
    "\n",
    "unified_fs = unified_fs.merge(\n",
    "    l1_short,\n",
    "    on=\"Feature\",\n",
    "    how=\"left\")\n",
    "\n",
    "# Merging Random Forest results\n",
    "\n",
    "rf_short = rf_results.rename(columns={\"Importance\": \"RF_importance\"})[\n",
    "    [\"Feature\", \"RF_importance\"]\n",
    "]\n",
    "\n",
    "unified_fs = unified_fs.merge(\n",
    "    rf_short,\n",
    "    on=\"Feature\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Merging XGBoost results\n",
    "\n",
    "xgb_short = xgb_results.rename(columns={\"Importance\": \"XGB_importance\"})[\n",
    "    [\"Feature\", \"XGB_importance\"]\n",
    "]\n",
    "\n",
    "unified_fs = unified_fs.merge(\n",
    "    xgb_short,\n",
    "    on=\"Feature\",\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping encoded features back to their raw feature name\n",
    "\n",
    "def get_raw_feature_name(encoded_feature):\n",
    "    \"\"\"\n",
    "    Map an encoded feature name (ord__/nom__/num__) back to the original column name.\n",
    "\n",
    "    Examples:\n",
    "      'ord__JobLevel'                      -> 'JobLevel'\n",
    "      'nom__BusinessTravel_Travel_Rarely'  -> 'BusinessTravel'\n",
    "      'nom__MaritalStatus_Single'          -> 'MaritalStatus'\n",
    "      'num__Age'                           -> 'Age'\n",
    "    \"\"\"\n",
    "    if encoded_feature.startswith(\"num__\") or encoded_feature.startswith(\"ord__\"):\n",
    "        # pattern: 'num__Age' or 'ord__JobLevel'\n",
    "        return encoded_feature.split(\"__\", 1)[1]\n",
    "\n",
    "    if encoded_feature.startswith(\"nom__\"):\n",
    "        # pattern: 'nom__Column_Category_With_Underscores'\n",
    "        tmp = encoded_feature.split(\"__\", 1)[1]  # 'Column_Category...'\n",
    "        raw_col = tmp.split(\"_\", 1)[0]           # take part before first '_'\n",
    "        return raw_col\n",
    "\n",
    "    # For any unexpected feature, return None\n",
    "    return None\n",
    "\n",
    "unified_fs[\"raw_feature\"] = unified_fs[\"Feature\"].apply(get_raw_feature_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging Chi-square results by raw feature\n",
    "\n",
    "chi2_short = chi2_results.rename(columns={\n",
    "    \"feature\": \"raw_feature\",\n",
    "    \"chi2\": \"chi2_stat\",\n",
    "    \"p_value\": \"chi2_pvalue\",\n",
    "    \"significant\": \"chi2_significant\"\n",
    "})[[\"raw_feature\", \"chi2_stat\", \"chi2_pvalue\", \"chi2_significant\"]]\n",
    "\n",
    "unified_fs = unified_fs.merge(\n",
    "    chi2_short,\n",
    "    on=\"raw_feature\",\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flagging whether feature is discrete (ordinal or one-hot)\n",
    "\n",
    "unified_fs[\"is_discrete\"] = unified_fs[\"Feature\"].str.startswith((\"ord__\", \"nom__\"))\n",
    "\n",
    "# Possible sorting: by Random Forest importance (descending)\n",
    "unified_fs_sorted = unified_fs.sort_values(\n",
    "    by=[\"RF_importance\", \"XGB_importance\", \"MI\"],\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "unified_fs_sorted.head(30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Finding which variables are consistently selected by the different feature selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = unified_fs.copy()  # keeping the original safe\n",
    "\n",
    "# Fill NaNs with 0 where it makes sense \n",
    "df[\"MI\"] = df[\"MI\"].fillna(0)\n",
    "df[\"RF_importance\"] = df[\"RF_importance\"].fillna(0)\n",
    "df[\"XGB_importance\"] = df[\"XGB_importance\"].fillna(0)\n",
    "\n",
    "# chi2_significant may be NaN for numeric features; treat those as False\n",
    "df[\"chi2_significant\"] = df[\"chi2_significant\"].fillna(False)\n",
    "\n",
    "# L1 Selected may be NaN for some features; treat as False\n",
    "df[\"Selected\"] = df[\"Selected\"].fillna(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Establishing dynamic thresholds (quantile-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to get a quantile threshold, but avoid NaNs / all-zeros issues\n",
    "def safe_quantile(series, q, default=0.0):\n",
    "    vals = series.dropna()\n",
    "    if (vals > 0).sum() == 0:\n",
    "        return default\n",
    "    return vals.quantile(q)\n",
    "\n",
    "# Example: top 30% for MI, RF, XGB\n",
    "mi_thresh  = safe_quantile(df[\"MI\"],             0.70, default=0.0)\n",
    "rf_thresh  = safe_quantile(df[\"RF_importance\"],  0.70, default=0.0)\n",
    "xgb_thresh = safe_quantile(df[\"XGB_importance\"], 0.70, default=0.0)\n",
    "\n",
    "print(\"MI threshold:\", mi_thresh)\n",
    "print(\"RF threshold:\", rf_thresh)\n",
    "print(\"XGB threshold:\", xgb_thresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Defining binary flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Chi-square (categoricals only) – already a boolean\n",
    "df[\"chi2_good\"] = df[\"chi2_significant\"].astype(bool)\n",
    "\n",
    "# 2) Mutual Information – above quantile threshold\n",
    "df[\"mi_good\"] = (df[\"MI\"] >= mi_thresh)\n",
    "\n",
    "# 3) L1 Logistic Regression – already boolean\n",
    "df[\"l1_good\"] = df[\"Selected\"].astype(bool)\n",
    "\n",
    "# 4) Random Forest – above quantile threshold\n",
    "df[\"rf_good\"] = (df[\"RF_importance\"] >= rf_thresh)\n",
    "\n",
    "# 5) XGBoost – above quantile threshold\n",
    "df[\"xgb_good\"] = (df[\"XGB_importance\"] >= xgb_thresh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Building the consensus score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_flags = [\"chi2_good\", \"mi_good\", \"l1_good\", \"rf_good\", \"xgb_good\"]\n",
    "\n",
    "# Converting to int and sum\n",
    "df[\"consensus_score\"] = df[method_flags].astype(int).sum(axis=1)\n",
    "\n",
    "# Checking distribution\n",
    "print(df[\"consensus_score\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Defining Feature Set A (strict) and Feature Set B (moderate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strict: features that get at least 3 \"votes\"\n",
    "strict_mask = df[\"consensus_score\"] >= 3\n",
    "\n",
    "# Moderate: at least 2 votes\n",
    "moderate_mask = df[\"consensus_score\"] >= 2\n",
    "\n",
    "# Feature names (encoded, ready for modelling)\n",
    "features_strict   = df.loc[strict_mask,   \"Feature\"].tolist()\n",
    "features_moderate = df.loc[moderate_mask, \"Feature\"].tolist()\n",
    "\n",
    "print(\"Number of features in strict set:\", len(features_strict))\n",
    "print(\"Number of features in moderate set:\", len(features_moderate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_consensus_view = df[[\n",
    "    \"Feature\",\n",
    "    \"raw_feature\",\n",
    "    \"MI\",\n",
    "    \"Coefficient\",\n",
    "    \"RF_importance\",\n",
    "    \"XGB_importance\",\n",
    "    \"chi2_stat\",\n",
    "    \"chi2_pvalue\",\n",
    "    \"chi2_good\",\n",
    "    \"mi_good\",\n",
    "    \"l1_good\",\n",
    "    \"rf_good\",\n",
    "    \"xgb_good\",\n",
    "    \"consensus_score\"\n",
    "]].sort_values(\"consensus_score\", ascending=False)\n",
    "\n",
    "df_consensus_view.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strict set of features\n",
    "\n",
    "for i, f in enumerate(features_strict, 1):\n",
    "    print(f\"{i}. {f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This strict set leans heavily toward a few thematic clusters:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Career progression & seniority (Age, TotalWorkingYears, YearsAtCompany, YearsWithCurrManager, JobLevel)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Income & compensation (MonthlyIncome, Income_Rate_Ratio, StockOptionLevel)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Job role (4–5 JobRole dummies)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - Marital status (Single / Divorced)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - OverTime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - EngagementIndex\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - BusinessTravel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " These are well known drivers of attrition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modearate set of features\n",
    "\n",
    "for i, f in enumerate(features_moderate, 1):\n",
    "    print(f\"{i}. {f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The moderate feature set as expected is more comprehensive and could be particularly useful for tree-based models. Of note, all engineered features are included in this set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following modelling function:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - accepts a feature list (strict or moderate)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - uses the pipeline_preprocess for encoding & scaling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - runs cross-validation on the training set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - evaluates several metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - trains the final model on the full training set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " - evaluates it on the held-out test set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " This will be the function we'll reuse for comparing different models as well as strict feature set vs moderate feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
    "\n",
    "# Turn a ColumnTransformer into a DataFrame with named columns\n",
    "class PreprocessToDF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ct):\n",
    "        self.ct = ct\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.ct.fit(X, y)\n",
    "        self.feature_names_ = self.ct.get_feature_names_out()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_arr = self.ct.transform(X)\n",
    "        index = getattr(X, \"index\", None)\n",
    "        return pd.DataFrame(X_arr, columns=self.feature_names_, index=index)\n",
    "\n",
    "\n",
    "# Select a subset of encoded features by name\n",
    "class ColumnSelectorByName(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, names=None):\n",
    "        # IMPORTANT: do NOT modify 'names' here; just store it\n",
    "        self.names = names\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # If names is None, select nothing\n",
    "        if self.names is None:\n",
    "            self.active_names_ = []\n",
    "        else:\n",
    "            # keep only those names that actually exist in this fold\n",
    "            self.active_names_ = [n for n in self.names if n in X.columns]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not hasattr(self, \"active_names_\"):\n",
    "            raise RuntimeError(\"ColumnSelectorByName is not fitted yet.\")\n",
    "        return X[self.active_names_]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_preprocessed(\n",
    "    model,\n",
    "    feature_list,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    pipeline_preprocess,\n",
    "    n_splits=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Leak-free evaluation:\n",
    "\n",
    "    - Builds Pipeline: preprocess → DataFrame → select(encoded cols) → scale → model\n",
    "    - Runs StratifiedKFold CV on RAW X_train (preprocessing inside CV)\n",
    "    - Fits final model on full X_train\n",
    "    - Tests once on untouched X_test\n",
    "    \"\"\"\n",
    "\n",
    "    # Clone unfitted ColumnTransformer from your original pipeline\n",
    "    ct = pipeline_preprocess.named_steps[\"preprocess\"]\n",
    "    ct = clone(ct)\n",
    "\n",
    "    # Build full modeling pipeline\n",
    "    base_pipe = Pipeline([\n",
    "        (\"preprocess\", PreprocessToDF(ct)),\n",
    "        (\"select\",    ColumnSelectorByName(feature_list)),\n",
    "        (\"scale\",     StandardScaler()),\n",
    "        (\"model\",     clone(model)),\n",
    "    ])\n",
    "\n",
    "    # Cross-validation\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    cv_acc, cv_prec, cv_rec, cv_f1 = [], [], [], []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[val_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        pipe_fold = clone(base_pipe)\n",
    "        pipe_fold.fit(X_tr, y_tr)\n",
    "        preds = pipe_fold.predict(X_val)\n",
    "\n",
    "        cv_acc.append(accuracy_score(y_val, preds))\n",
    "        cv_prec.append(precision_score(y_val, preds))\n",
    "        cv_rec.append(recall_score(y_val, preds))\n",
    "        cv_f1.append(f1_score(y_val, preds))\n",
    "\n",
    "    # Final fit on full training set\n",
    "    final_pipe = clone(base_pipe)\n",
    "    final_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Test evaluation (still untouched)\n",
    "    test_preds = final_pipe.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        \"cv_accuracy\":    np.mean(cv_acc),\n",
    "        \"cv_precision\":   np.mean(cv_prec),\n",
    "        \"cv_recall\":      np.mean(cv_rec),\n",
    "        \"cv_f1\":          np.mean(cv_f1),\n",
    "        \"test_accuracy\":  accuracy_score(y_test, test_preds),\n",
    "        \"test_precision\": precision_score(y_test, test_preds),\n",
    "        \"test_recall\":    recall_score(y_test, test_preds),\n",
    "        \"test_f1\":        f1_score(y_test, test_preds),\n",
    "        \"fitted_pipeline\": final_pipe,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model factory functions: each call returns a FRESH instance ---\n",
    "\n",
    "def make_lr():\n",
    "    return LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=500,\n",
    "        solver='lbfgs',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def make_rf():\n",
    "    return RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "def make_xgb():\n",
    "    # handle imbalance with scale_pos_weight\n",
    "    pos = y_train.sum()\n",
    "    neg = len(y_train) - pos\n",
    "    spw = neg / pos\n",
    "    \n",
    "    return XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        scale_pos_weight=spw,\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "def make_mlp():\n",
    "    return MLPClassifier(\n",
    "        hidden_layer_sizes=(32,),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=1e-4,\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def make_svc():\n",
    "    return SVC(\n",
    "        kernel='rbf',\n",
    "        C=1.0,\n",
    "        gamma='scale',\n",
    "        probability=True,         # in case later want predict_proba\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "def make_lgbm():\n",
    "    return LGBMClassifier(\n",
    "        n_estimators=400,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=-1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "def make_cat():\n",
    "    return CatBoostClassifier(\n",
    "        iterations=400,\n",
    "        depth=4,\n",
    "        learning_rate=0.1,\n",
    "        loss_function='Logloss',\n",
    "        eval_metric='F1',\n",
    "        auto_class_weights='Balanced',\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_factories = {\n",
    "    \"LR\":   make_lr,\n",
    "    \"RF\":   make_rf,\n",
    "    \"XGB\":  make_xgb,\n",
    "    \"MLP\":  make_mlp,\n",
    "    \"SVC\":  make_svc,\n",
    "    \"LGBM\": make_lgbm,\n",
    "    \"CAT\":  make_cat\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "    \"strict\":   features_strict,\n",
    "    \"moderate\": features_moderate\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = []\n",
    "\n",
    "for model_name, factory in model_factories.items():\n",
    "    for fs_name, fs_list in feature_sets.items():\n",
    "        print(f\"Running {model_name} with {fs_name} features...\")\n",
    "        \n",
    "        model = factory()  # FRESH instance each time\n",
    "        \n",
    "        res = evaluate_model_preprocessed(\n",
    "            model=model,\n",
    "            feature_list=fs_list,\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            X_test=X_test, y_test=y_test,\n",
    "            pipeline_preprocess=pipeline_preprocess\n",
    "        )\n",
    "        \n",
    "        # add identifiers\n",
    "        res[\"model\"] = model_name\n",
    "        res[\"feature_set\"] = fs_name\n",
    "        \n",
    "        baseline_results.append(res)\n",
    "\n",
    "# Convert to DataFrame for nice viewing\n",
    "results_df = pd.DataFrame(baseline_results)\n",
    "\n",
    "# Put columns in a convenient order\n",
    "cols_order = [\n",
    "    \"model\", \"feature_set\",\n",
    "    \"cv_accuracy\", \"cv_precision\", \"cv_recall\", \"cv_f1\",\n",
    "    \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"\n",
    "]\n",
    "results_df = results_df[cols_order]\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Based on the baseline results, XGBoost paired with the moderate feature set provides one of the most favourable trade-offs across all evaluation metrics. This makes it our best candidate for further optimisation, so we will focus our hyperparameter tuning (using GridSearch and Optuna) on this configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Hyperparameter tuning using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We'll reuse our evaluate_model_preprocessed while looping over a small grid to assess:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - which max_depth generally works best\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - whether smaller/larger learning_rate helps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - whether more trees improve things\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    - whether subsampling is beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to compute imbalance weight\n",
    "\n",
    "def compute_spw(y):\n",
    "    pos = y.sum()\n",
    "    neg = len(y) - pos\n",
    "    return neg / pos\n",
    "\n",
    "spw = compute_spw(y_train)\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\":      [3, 4, 5],\n",
    "    \"learning_rate\":  [0.05, 0.1],\n",
    "    \"n_estimators\":   [200, 400],\n",
    "    \"subsample\":      [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid_results = []\n",
    "\n",
    "for max_depth, lr, n_est, subs, colsub in product(\n",
    "    param_grid[\"max_depth\"],\n",
    "    param_grid[\"learning_rate\"],\n",
    "    param_grid[\"n_estimators\"],\n",
    "    param_grid[\"subsample\"],\n",
    "    param_grid[\"colsample_bytree\"],\n",
    "):\n",
    "    print(f\"Testing: depth={max_depth}, lr={lr}, n_est={n_est}, subs={subs}, col={colsub}\")\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=lr,\n",
    "        n_estimators=n_est,\n",
    "        subsample=subs,\n",
    "        colsample_bytree=colsub,\n",
    "        scale_pos_weight=spw,\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",  # faster if available\n",
    "    )\n",
    "\n",
    "    res = evaluate_model_preprocessed(\n",
    "        model=model,\n",
    "        feature_list=features_moderate,\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        X_test=X_test, y_test=y_test,\n",
    "        pipeline_preprocess=pipeline_preprocess\n",
    "    )\n",
    "\n",
    "    res[\"max_depth\"] = max_depth\n",
    "    res[\"learning_rate\"] = lr\n",
    "    res[\"n_estimators\"] = n_est\n",
    "    res[\"subsample\"] = subs\n",
    "    res[\"colsample_bytree\"] = colsub\n",
    "\n",
    "    grid_results.append(res)\n",
    "\n",
    "grid_df = pd.DataFrame(grid_results)\n",
    "\n",
    "# sort by CV F1 (primary) then test F1 (secondary)\n",
    "grid_df_sorted = grid_df.sort_values(\n",
    "    by=[\"cv_f1\", \"test_f1\"], ascending=False\n",
    ").reset_index(drop=True)\n",
    "\n",
    "grid_df_sorted.head(10)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best_results = grid_df_sorted.iloc[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    By performing grid search we obtained modest but consistent improvements over the baseline XGBoost model in cross-validation performance, particularly in precision and F1. Test-set results remain close to the baseline, indicating that the model is stable and not highly sensitive to the grid’s parameter variations. These results justify trying Optuna in order to explore the hyperparameter space more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Hyperparameter tuning using Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We define our Optuna search space based on the information we obtained from the (coarse) grid search above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from itertools import product\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Best row from your grid search (already sorted by cv_f1 desc)\n",
    "best_row = grid_df_sorted.iloc[0]\n",
    "\n",
    "best_md   = int(best_row[\"max_depth\"])\n",
    "best_lr   = float(best_row[\"learning_rate\"])\n",
    "best_ne   = int(best_row[\"n_estimators\"])\n",
    "best_sub  = float(best_row[\"subsample\"])\n",
    "best_col  = float(best_row[\"colsample_bytree\"])\n",
    "\n",
    "print(\"Best grid params (CV):\")\n",
    "print(\"max_depth:\", best_md)\n",
    "print(\"learning_rate:\", best_lr)\n",
    "print(\"n_estimators:\", best_ne)\n",
    "print(\"subsample:\", best_sub)\n",
    "print(\"colsample_bytree:\", best_col)\n",
    "\n",
    "# Helper to keep ranges reasonable\n",
    "def clip_int(low, high, min_val, max_val):\n",
    "    return max(min_val, low), min(max_val, high)\n",
    "\n",
    "def clip_float(low, high, min_val, max_val):\n",
    "    return max(min_val, low), min(max_val, high)\n",
    "\n",
    "# Ranges centred around grid best (you can tweak)\n",
    "md_low, md_high     = clip_int(best_md - 1, best_md + 1, 2, 8)\n",
    "ne_low, ne_high     = clip_int(best_ne - 100, best_ne + 200, 100, 800)\n",
    "sub_low, sub_high   = clip_float(best_sub - 0.2, best_sub + 0.2, 0.5, 1.0)\n",
    "col_low, col_high   = clip_float(best_col - 0.2, best_col + 0.2, 0.5, 1.0)\n",
    "\n",
    "lr_low  = max(0.01, best_lr / 3)\n",
    "lr_high = min(0.3,  best_lr * 3)\n",
    "\n",
    "print(\"\\nOptuna search ranges:\")\n",
    "print(\"max_depth:\", (md_low, md_high))\n",
    "print(\"learning_rate:\", (lr_low, lr_high))\n",
    "print(\"n_estimators:\", (ne_low, ne_high))\n",
    "print(\"subsample:\", (sub_low, sub_high))\n",
    "print(\"colsample_bytree:\", (col_low, col_high))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Optuna objective + study (optimising CV F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n",
    "\n",
    "    max_depth = trial.suggest_int(\"max_depth\", md_low, md_high)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", lr_low, lr_high, log=True)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", ne_low, ne_high)\n",
    "    subsample = trial.suggest_float(\"subsample\", sub_low, sub_high)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", col_low, col_high)\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        scale_pos_weight=spw,          # your imbalance weight\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "\n",
    "    res = evaluate_model_preprocessed(\n",
    "        model=model,\n",
    "        feature_list=features_moderate,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        pipeline_preprocess=pipeline_preprocess,\n",
    "        n_splits=5\n",
    "    )\n",
    "\n",
    "    # We optimise CV F1 (NOT test F1)\n",
    "    return res[\"cv_f1\"]\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=\"xgb_moderate_optuna\",\n",
    "    sampler=TPESampler(seed=42),\n",
    ")\n",
    "\n",
    "study.optimize(xgb_objective, n_trials=40, show_progress_bar=True)\n",
    "\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial CV F1:\", study.best_value)\n",
    "print(\"Best trial params:\", study.best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train + evaluate final model with best Optuna params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we build a fresh XGB model with the best Optuna params, run it through the same evaluation function, and inspect both CV + test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(\"\\nBest Optuna params:\")\n",
    "for k, v in best_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "best_xgb = XGBClassifier(\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    subsample=best_params[\"subsample\"],\n",
    "    colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "    scale_pos_weight=spw,\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    ")\n",
    "\n",
    "optuna_results = evaluate_model_preprocessed(\n",
    "    model=best_xgb,\n",
    "    feature_list=features_moderate,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    pipeline_preprocess=pipeline_preprocess,\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "print(\"\\nFinal XGBoost (Optuna) performance:\")\n",
    "for k, v in optuna_results.items():\n",
    "    if k != \"fitted_pipeline\":   # don't spam with the whole pipeline object\n",
    "        print(f\"{k}: {v}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_model(name, feature_set, row_or_results):\n",
    "    \"\"\"row_or_results can be a pandas row (baseline) or the optuna_results dict.\"\"\"\n",
    "    if isinstance(row_or_results, dict):\n",
    "        # optuna_results style\n",
    "        return {\n",
    "            \"model\": name,\n",
    "            \"feature_set\": feature_set,\n",
    "            \"cv_accuracy\":  row_or_results[\"cv_accuracy\"],\n",
    "            \"cv_precision\": row_or_results[\"cv_precision\"],\n",
    "            \"cv_recall\":    row_or_results[\"cv_recall\"],\n",
    "            \"cv_f1\":        row_or_results[\"cv_f1\"],\n",
    "            \"test_accuracy\":  row_or_results[\"test_accuracy\"],\n",
    "            \"test_precision\": row_or_results[\"test_precision\"],\n",
    "            \"test_recall\":    row_or_results[\"test_recall\"],\n",
    "            \"test_f1\":        row_or_results[\"test_f1\"],\n",
    "        }\n",
    "    else:\n",
    "        # pandas Series from results_df\n",
    "        r = row_or_results\n",
    "        return {\n",
    "            \"model\": name,\n",
    "            \"feature_set\": feature_set,\n",
    "            \"cv_accuracy\":  r[\"cv_accuracy\"],\n",
    "            \"cv_precision\": r[\"cv_precision\"],\n",
    "            \"cv_recall\":    r[\"cv_recall\"],\n",
    "            \"cv_f1\":        r[\"cv_f1\"],\n",
    "            \"test_accuracy\":  r[\"test_accuracy\"],\n",
    "            \"test_precision\": r[\"test_precision\"],\n",
    "            \"test_recall\":    r[\"test_recall\"],\n",
    "            \"test_f1\":        r[\"test_f1\"],\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline XGB with moderate features\n",
    "xgb_base_mod = results_df.query(\"model == 'XGB' and feature_set == 'moderate'\").iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "rows.append(summarize_model(\"XGB_baseline\", \"moderate\", xgb_base_mod))\n",
    "rows.append(summarize_model(\"XGB_GridBest\", \"moderate\", grid_best_results))\n",
    "rows.append(summarize_model(\"XGB_Optuna\", \"moderate\", optuna_results))\n",
    "\n",
    "comparison_df = pd.DataFrame(rows)\n",
    "comparison_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Tuning with Optuna didn’t beat the baseline - test_f1 Optuna 0.44 vs test_f1 base model 0.49\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    After tuning XGBoost with 2 different approaches, we conclude that the baseline configuration was already near the performance ceiling for this dataset; additional tuning brought only marginal changes in test F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    For model evaluation we'll use our XGB base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    f1_score\n",
    ")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit your BASELINE model using the leak-free evaluator\n",
    "baseline_xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=spw,   # your imbalance weight computed earlier\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    ")\n",
    "\n",
    "baseline_results = evaluate_model_preprocessed(\n",
    "    model=baseline_xgb,\n",
    "    feature_list=features_moderate,   # or features_strict if you prefer\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    pipeline_preprocess=pipeline_preprocess,\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "print(\"Baseline XGB (moderate) CV/Test metrics:\")\n",
    "for k, v in baseline_results.items():\n",
    "    if k != \"fitted_pipeline\":\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "# grab the fitted full pipeline\n",
    "baseline_pipe = baseline_results[\"fitted_pipeline\"]\n",
    "\n",
    "\n",
    "# 2Get predicted probabilities on the TEST set\n",
    "\n",
    "y_test_proba = baseline_pipe.predict_proba(X_test)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve + AUC\n",
    "\n",
    "fpr, tpr, roc_thresh = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", alpha=0.7)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – Baseline XGBoost (moderate)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The XGBoost model achieves a ROC-AUC of 0.761, indicating good ability to discriminate between employees who leave and those who stay. The curve rises steeply at low false-positive rates, showing that the model correctly identifies many true quitters before confusing them with non-quitters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    However, ROC-AUC evaluates performance across all possible thresholds and is insensitive to class imbalance. In an attrition scenario where only ~15% of employees leave, a model can obtain a relatively high ROC-AUC even when the practical precision and recall trade-off is challenging.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Therefore, while the ROC curve confirms that the model produces a meaningful ranking of risk scores, it must be interpreted alongside the Precision–Recall curve and F1 score to fully understand real-world classification performance under imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision–Recall curve + AP\n",
    "\n",
    "prec, rec, pr_thresh = precision_recall_curve(y_test, y_test_proba)\n",
    "ap = average_precision_score(y_test, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(rec, prec, label=f\"AP = {ap:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve – Baseline XGBoost (moderate)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Precision–Recall curve provides a more realistic evaluation of model performance under class imbalance than ROC-AUC. The model achieves an Average Precision (AP) of 0.5, substantially higher than the baseline positive rate of ~0.15, confirming that it captures meaningful signal related to employee attrition. Precision is extremely high at low recall (0.85–1.00), indicating that the model is very confident in identifying the top-risk employees. As recall increases, precision decreases, reflecting a typical precision–recall tradeoff in imbalanced datasets, where achieving high recall requires accepting more false positives. These results align with the observed F1 scores (0.45–0.49), confirming that the model captures meaningful signal but cannot achieve high precision and recall simultaneously.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Although the model ranks employees well (as shown by the ROC curve), achieving strong precision and recall simultaneously is challenging due to the underlying class imbalance. The PR curve therefore offers a realistic assessment of actionable performance and complements the ROC curve by showing where the model can be most effectively used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold vs F1 on the TEST set (diagnostic)\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.95, 181)  # step = 0.005\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds_t = (y_test_proba >= t).astype(int)\n",
    "    f1_scores.append(f1_score(y_test, preds_t))\n",
    "\n",
    "f1_scores = np.array(f1_scores)\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_t = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(thresholds, f1_scores, label=\"F1 score\")\n",
    "plt.axvline(0.5, color=\"red\", linestyle=\"--\", label=\"Default 0.5\")\n",
    "plt.axvline(best_t, color=\"green\", linestyle=\"--\",\n",
    "            label=f\"Best on test: t={best_t:.3f}, F1={best_f1:.3f}\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.title(\"Threshold vs F1 – Baseline XGBoost (moderate)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The Threshold–F1 curve illustrates how the model’s F1 score changes as the decision threshold varies between 0 and 1. The model achieves its highest F1 values—peaking just above 0.50, which is when the threshold is in the 0.53 to 0.54 range. This means that, on the test set, increasing the threshold just a notch above the default 0.50 improves the balance between precision and recall.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    However, this peak reflects test-set behavior and should not be used for model selection, as tuning a threshold on test data introduces data leakage.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Overall, the curve highlights how sensitive F1 performance is to threshold choice, especially in imbalanced datasets, and reinforces the importance of selecting thresholds based on training-only validation procedures rather than test-set optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Threshold Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Threshold–F1 curve above shows that the model’s performance varies substantially across different probability cutoffs. Although the default threshold of 0.50 is commonly used, it is not the point that maximizes the F1 score on this dataset. This variation suggests that adjusting the classification threshold could meaningfully improve the balance between precision and recall.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Therefore, in the next section, we perform a dedicated threshold optimization procedure using using validation performed only on the training data (avoiding leakage) in an attempt identify a more effective decision threshold for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find best threshold on one set\n",
    "\n",
    "def find_best_threshold(y_true, y_proba, metric=f1_score, thresholds=None):\n",
    "    \"\"\"\n",
    "    Given true labels and predicted probabilities, find the threshold\n",
    "    that maximizes the chosen metric (default: F1).\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 181)  # step = 0.005\n",
    "\n",
    "    scores = []\n",
    "    for t in thresholds:\n",
    "        preds = (y_proba >= t).astype(int)\n",
    "        scores.append(metric(y_true, preds))\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    best_idx = np.argmax(scores)\n",
    "    return float(thresholds[best_idx]), float(scores[best_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to evaluate model with CV threshold tuning\n",
    "\n",
    "def evaluate_model_with_threshold_cv(\n",
    "    model,\n",
    "    feature_list,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    pipeline_preprocess,\n",
    "    n_splits=5,\n",
    "    metric=f1_score\n",
    "):\n",
    "    \"\"\"\n",
    "    Leak-free evaluation with threshold optimization via cross-validation.\n",
    "\n",
    "    - Pipeline: preprocess -> DataFrame -> select(encoded cols) -> scale -> model\n",
    "    - For each CV fold:\n",
    "        * fit pipeline on fold's train\n",
    "        * get predict_proba on fold's val\n",
    "        * find best threshold on that fold (for chosen metric, default F1)\n",
    "    - Global threshold T* = mean of per-fold best thresholds\n",
    "    - Final pipeline fit on full X_train\n",
    "    - Test metrics computed using T* on predict_proba(X_test)\n",
    "    \"\"\"\n",
    "\n",
    "    # Clone unfitted ColumnTransformer from your pipeline_preprocess\n",
    "    ct = pipeline_preprocess.named_steps[\"preprocess\"]\n",
    "    ct = clone(ct)\n",
    "\n",
    "    # Base pipeline (unfitted)\n",
    "    base_pipe = Pipeline([\n",
    "        (\"preprocess\", PreprocessToDF(ct)),\n",
    "        (\"select\",    ColumnSelectorByName(feature_list)),\n",
    "        (\"scale\",     StandardScaler()),\n",
    "        (\"model\",     clone(model)),\n",
    "    ])\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_thresholds = []\n",
    "    cv_metrics = {\n",
    "        \"accuracy\": [],\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1\": []\n",
    "    }\n",
    "\n",
    "    # CV loop with threshold tuning \n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr = X_train.iloc[train_idx]\n",
    "        y_tr = y_train.iloc[train_idx]\n",
    "        X_val = X_train.iloc[val_idx]\n",
    "        y_val = y_train.iloc[val_idx]\n",
    "\n",
    "        pipe_fold = clone(base_pipe)\n",
    "        pipe_fold.fit(X_tr, y_tr)\n",
    "\n",
    "        # probabilities on validation fold\n",
    "        y_val_proba = pipe_fold.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # best threshold on this fold\n",
    "        t_fold, _ = find_best_threshold(y_val, y_val_proba, metric=metric)\n",
    "        fold_thresholds.append(t_fold)\n",
    "\n",
    "        # apply this fold's threshold to compute metrics\n",
    "        y_val_pred = (y_val_proba >= t_fold).astype(int)\n",
    "\n",
    "        cv_metrics[\"accuracy\"].append(accuracy_score(y_val, y_val_pred))\n",
    "        cv_metrics[\"precision\"].append(precision_score(y_val, y_val_pred))\n",
    "        cv_metrics[\"recall\"].append(recall_score(y_val, y_val_pred))\n",
    "        cv_metrics[\"f1\"].append(f1_score(y_val, y_val_pred))\n",
    "\n",
    "    # Global threshold = mean of best thresholds\n",
    "    T_star = float(np.mean(fold_thresholds))\n",
    "\n",
    "    # Final fit on full training data \n",
    "    final_pipe = clone(base_pipe)\n",
    "    final_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Test evaluation using T_star \n",
    "    y_test_proba = final_pipe.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = (y_test_proba >= T_star).astype(int)\n",
    "\n",
    "    test_results = {\n",
    "        \"test_accuracy\":  accuracy_score(y_test, y_test_pred),\n",
    "        \"test_precision\": precision_score(y_test, y_test_pred),\n",
    "        \"test_recall\":    recall_score(y_test, y_test_pred),\n",
    "        \"test_f1\":        f1_score(y_test, y_test_pred),\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        \"cv_accuracy\":   float(np.mean(cv_metrics[\"accuracy\"])),\n",
    "        \"cv_precision\":  float(np.mean(cv_metrics[\"precision\"])),\n",
    "        \"cv_recall\":     float(np.mean(cv_metrics[\"recall\"])),\n",
    "        \"cv_f1\":         float(np.mean(cv_metrics[\"f1\"])),\n",
    "        \"cv_thresholds\": fold_thresholds,\n",
    "        \"best_threshold\": T_star,\n",
    "        \"test_accuracy\":  test_results[\"test_accuracy\"],\n",
    "        \"test_precision\": test_results[\"test_precision\"],\n",
    "        \"test_recall\":    test_results[\"test_recall\"],\n",
    "        \"test_f1\":        test_results[\"test_f1\"],\n",
    "        \"fitted_pipeline\": final_pipe,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Applying to our baseline XGBoost (moderate features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_xgb = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=spw,\n",
    "    random_state=42,\n",
    "    eval_metric=\"logloss\",\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    ")\n",
    "\n",
    "xgb_threshold_results = evaluate_model_with_threshold_cv(\n",
    "    model=baseline_xgb,\n",
    "    feature_list=features_moderate,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    pipeline_preprocess=pipeline_preprocess,\n",
    "    n_splits=5,\n",
    "    metric=f1_score   # you can swap to recall_score if you prefer\n",
    ")\n",
    "\n",
    "print(\"Cross-validated threshold tuning results (XGB, moderate):\")\n",
    "for k, v in xgb_threshold_results.items():\n",
    "    if k not in (\"fitted_pipeline\", \"cv_thresholds\"):\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\nPer-fold thresholds:\", xgb_threshold_results[\"cv_thresholds\"])\n",
    "print(\"Global T* (mean threshold):\", xgb_threshold_results[\"best_threshold\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Using stratified K-fold cross-validation, the threshold-optimization procedure identified a threshold of 0.49. When this cross-validated threshold was applied to the test set, the model achieved performance metrics very similar to those obtained with the 0.5 threshold, which tells us that, for this dataset, adjusting the decision threshold does not result in an improvement over the baseline setting. This is not surprising considering how close the optimized and the base threshold are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    accuracy_score, confusion_matrix\n",
    ")\n",
    "\n",
    "\n",
    "# Use fitted baseline pipeline & get test probabilities from earlier baseline evaluation:\n",
    "# baseline_results = evaluate_model_preprocessed(...)\n",
    "\n",
    "final_pipeline = baseline_results[\"fitted_pipeline\"]\n",
    "\n",
    "# Probabilities for the test set\n",
    "y_test_proba = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# Final threshold\n",
    "\n",
    "final_threshold = 0.50   # default or your chosen business threshold\n",
    "\n",
    "# Final predictions\n",
    "y_pred = (y_test_proba >= final_threshold).astype(int)\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "\n",
    "test_accuracy  = accuracy_score(y_test, y_pred)\n",
    "test_precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "test_recall    = recall_score(y_test, y_pred)\n",
    "test_f1        = f1_score(y_test, y_pred)\n",
    "\n",
    "metrics_table = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
    "    \"Score\": [test_accuracy, test_precision, test_recall, test_f1]\n",
    "})\n",
    "\n",
    "print(f\"Final Classification Metrics (threshold = {final_threshold:.2f})\")\n",
    "display(metrics_table)\n",
    "\n",
    "\n",
    "# Confusion Matrix (Heatmap)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Predicted No Attrition\", \"Predicted Attrition\"],\n",
    "    yticklabels=[\"Actual No Attrition\", \"Actual Attrition\"]\n",
    ")\n",
    "plt.title(f\"Confusion Matrix (threshold = {final_threshold:.2f})\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Model Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## SHAP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To better understand how our XGBoost model makes predictions, we now turn to SHAP (SHapley Additive exPlanations). SHAP sheds light on the \"black box\" behind predictions by showing how each feature influences the model’s output, both at the dataset level and for individual employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Compute SHAP values (TreeExplainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1. Pull pieces out of your fitted baseline pipeline\n",
    "# -------------------------------------------------\n",
    "final_pipeline = baseline_results[\"fitted_pipeline\"]\n",
    "\n",
    "preprocess_df = final_pipeline.named_steps[\"preprocess\"]   # PreprocessToDF\n",
    "selector      = final_pipeline.named_steps[\"select\"]       # ColumnSelectorByName\n",
    "model         = final_pipeline.named_steps[\"model\"]        # XGBClassifier\n",
    "\n",
    "# Encode + select TRAIN features (same as model sees)\n",
    "X_train_encoded = preprocess_df.transform(X_train)         # DataFrame with encoded cols\n",
    "encoded_feature_names = selector.active_names_             # selected encoded feature names\n",
    "X_train_sel = X_train_encoded[encoded_feature_names]\n",
    "\n",
    "# Optional: smaller background for speed\n",
    "background = X_train_sel.sample(n=min(200, len(X_train_sel)), random_state=42)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. TreeExplainer on encoded features\n",
    "# -------------------------------------------------\n",
    "explainer = shap.TreeExplainer(model, data=background)\n",
    "shap_values = explainer.shap_values(X_train_sel)\n",
    "\n",
    "# For binary XGBoost, TreeExplainer returns list [class0, class1]\n",
    "shap_values_pos = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. RAW SHAP PLOTS (encoded features) – optional, for you\n",
    "# -------------------------------------------------\n",
    "shap.summary_plot(\n",
    "    shap_values_pos,\n",
    "    X_train_sel,\n",
    "    feature_names=encoded_feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    show=True\n",
    ")\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_pos,\n",
    "    X_train_sel,\n",
    "    feature_names=encoded_feature_names,\n",
    "    show=True\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4. Helper: map encoded name -> raw feature name\n",
    "# -------------------------------------------------\n",
    "def get_raw_name(encoded_name):\n",
    "    \"\"\"\n",
    "    'nom__BusinessTravel_Travel_Frequently' -> 'BusinessTravel'\n",
    "    'ord__JobLevel'                         -> 'JobLevel'\n",
    "    'num__Age'                              -> 'Age'\n",
    "    \"\"\"\n",
    "    name = encoded_name.split(\"__\", 1)[1]   # strip num__/ord__/nom__\n",
    "    if \"_\" in name:                         # one-hot: raw_feature_category\n",
    "        return name.split(\"_\", 1)[0]\n",
    "    return name\n",
    "\n",
    "raw_names = [get_raw_name(f) for f in encoded_feature_names]\n",
    "unique_raw = sorted(set(raw_names))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5. Group SHAP values by raw feature\n",
    "# -------------------------------------------------\n",
    "# Group SHAP values: sum impacts of all one-hot columns per raw feature\n",
    "grouped_shap_df = (\n",
    "    pd.DataFrame(shap_values_pos, columns=encoded_feature_names)\n",
    "      .groupby(raw_names, axis=1)\n",
    "      .sum()\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6. Build grouped feature values (for beeswarm colors)\n",
    "# -------------------------------------------------\n",
    "grouped_features_dict = {}\n",
    "\n",
    "for raw in unique_raw:\n",
    "    # encoded columns belonging to this raw feature\n",
    "    cols = [c for c, r in zip(encoded_feature_names, raw_names) if r == raw]\n",
    "\n",
    "    if len(cols) == 1:\n",
    "        # ordinal or numeric → keep as-is\n",
    "        grouped_features_dict[raw] = X_train_sel[cols[0]].values\n",
    "    else:\n",
    "        # multi-category one-hot (e.g., BusinessTravel, JobRole)\n",
    "        # represent category as argmax index across dummies\n",
    "        encoded = X_train_sel[cols].values\n",
    "        grouped_features_dict[raw] = encoded.argmax(axis=1)\n",
    "\n",
    "# Overwrite OverTime with the original binary column so colors make sense\n",
    "if \"OverTime\" in X_train.columns:\n",
    "    grouped_features_dict[\"OverTime\"] = X_train[\"OverTime\"].values\n",
    "\n",
    "# Build DataFrame with same index as X_train\n",
    "grouped_features = pd.DataFrame(grouped_features_dict, index=X_train.index)\n",
    "\n",
    "# Align column order with grouped_shap_df just in case\n",
    "grouped_features = grouped_features[grouped_shap_df.columns]\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 7. GROUPED SHAP PLOTS – for lay audience\n",
    "# -------------------------------------------------\n",
    "# Bar plot (global importance)\n",
    "shap.summary_plot(\n",
    "    grouped_shap_df.values,\n",
    "    grouped_features,\n",
    "    feature_names=grouped_shap_df.columns,\n",
    "    plot_type=\"bar\",\n",
    "    show=True\n",
    ")\n",
    "\n",
    "# Beeswarm plot (direction + distribution)\n",
    "shap.summary_plot(\n",
    "    grouped_shap_df.values,\n",
    "    grouped_features,\n",
    "    feature_names=grouped_shap_df.columns,\n",
    "    show=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Bar plot: feature importance ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This SHAP summary plot highlights the features that contribute most to the model’s attrition predictions. The most influential factors include OverTime, StockOptionLevel, EngagementIndex, Age, and MonthlyIncome, indicating that workload, compensation, engagement, and career stage play central roles in predicting whether an employee will leave. Several job-related and financial features, such as JobRole (Research Scientist), NumCompaniesWorked, DailyRate, and Income vs. Role Median also show meaningful impact. Meanwhile, features related to satisfaction, work–life balance, distance from home, and duration under current manager contribute moderately but consistently. Lower-ranked features still affect predictions, but with smaller average influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Beeswarm: direction + magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This SHAP beeswarm plot provides a detailed view of how individual feature values influence the model’s predictions for employee attrition. Features appear in order of overall importance, and each point represents an employee. The color indicates whether the feature value is high (pink) or low (blue), while the position on the x-axis shows whether that value increases or decreases the predicted probability of quitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  A few patterns stand out clearly:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  - Low OverTime (0) strongly reduces attrition risk, while high OverTime tends to push predictions toward quitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  - Lower StockOptionLevel, lower EngagementIndex, and younger Age similarly drive predictions upward, indicating higher risk.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  - Higher MonthlyIncome and higher IncomeVsRoleMedian generally push predictions downward, aligning with retention.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  - For several features—such as NumCompaniesWorked, DailyRate, and DistanceFromHome—both high and low values can influence the prediction direction, reflecting more complex, nonlinear relationships the model has learned.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  Overall, this plot shows not just which features matter most, but also how specific feature values contribute to individual attrition predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Interaction plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  This SHAP dependence plot shows how the EngagementIndex influences the model’s prediction of attrition, while also highlighting its interaction with OverTime. As EngagementIndex increases, SHAP values clearly decrease, indicating that more engaged employees are predicted to be at lower risk of quitting. Conversely, lower engagement strongly pushes the model toward predicting attrition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  The color scale reveals an interaction effect: employees who do not work overtime (blue) generally have slightly lower SHAP values at similar engagement levels, reinforcing a lower predicted risk. Those who do work overtime (pink) tend to contribute more positively to the attrition prediction, even when engagement is moderate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  Overall, the plot shows a clean, monotonic relationship: lower engagement → higher attrition risk, with overtime amplifying this effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # SVC hyperparameter tuning using randomized search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  From the base models we trained, our second best was a Support Vector Machine model using the restricted set of features. Let's try to optimize it and see how it compares with the XGBoost model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We start by tuning it with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "def make_svc():\n",
    "    \"\"\"Fresh SVC instance.\"\"\"\n",
    "    return SVC(\n",
    "        kernel=\"rbf\",\n",
    "        probability=True,         # needed for ROC, PR, threshold tuning\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "\n",
    "def tune_svc_random_search(\n",
    "    feature_list,\n",
    "    X_train, y_train,\n",
    "    pipeline_preprocess,\n",
    "    n_splits=5,\n",
    "    n_iter=40\n",
    "):\n",
    "    \"\"\"\n",
    "    Leak-free SVC tuner:\n",
    "    - Uses same pattern as evaluate_model_preprocessed:\n",
    "      preprocess (ColumnTransformer) -> DF -> select -> scale -> model\n",
    "    - Wraps everything in a Pipeline and tunes only SVC hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Clone the ColumnTransformer from your preprocessing pipeline\n",
    "    ct = pipeline_preprocess.named_steps[\"preprocess\"]\n",
    "    ct = clone(ct)\n",
    "\n",
    "    # 2) Build full modeling pipeline (same structure as eval function)\n",
    "    base_pipe = Pipeline([\n",
    "        (\"preprocess\", PreprocessToDF(ct)),           # returns DataFrame with feature names\n",
    "        (\"select\",    ColumnSelectorByName(feature_list)),\n",
    "        (\"scale\",     StandardScaler()),\n",
    "        (\"model\",     make_svc()),\n",
    "    ])\n",
    "\n",
    "    # 3) Define search space on the *model* step\n",
    "    param_dist = {\n",
    "        \"model__C\":     np.logspace(-2, 3, 30),   # 0.01 → 1000\n",
    "        \"model__gamma\": np.logspace(-4, 1, 30),   # 1e-4 → 10\n",
    "        \"model__kernel\": [\"rbf\"],                # could add \"linear\" if desired\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base_pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=n_iter,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "    # 4) Fit search on RAW training data (pipeline handles transforms inside CV)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best SVC parameters:\", search.best_params_)\n",
    "    print(\"Best CV F1 (RandomizedSearchCV):\", search.best_score_)\n",
    "\n",
    "    # best_estimator_ is the full pipeline\n",
    "    return search.best_estimator_, search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_features = features_strict   \n",
    "\n",
    "best_svc_pipeline, svc_search = tune_svc_random_search(\n",
    "    feature_list=svc_features,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    pipeline_preprocess=pipeline_preprocess,\n",
    "    n_splits=5,\n",
    "    n_iter=40\n",
    ")\n",
    "\n",
    "# Evaluate using the same leak-free evaluator\n",
    "svc_tuned_results = evaluate_model_preprocessed(\n",
    "    model=best_svc_pipeline.named_steps[\"model\"],  # tuned SVC\n",
    "    feature_list=svc_features,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,   y_test=y_test,\n",
    "    pipeline_preprocess=pipeline_preprocess\n",
    ")\n",
    "\n",
    "svc_tuned_results[\"model\"] = \"SVC_tuned\"\n",
    "svc_tuned_results[\"feature_set\"] = \"strict\"\n",
    "\n",
    "pd.DataFrame([svc_tuned_results])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # SVC hyperparameter tuning using grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  We used the best parameters identified in the randomized search to guide the selection of values explored in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def tune_svc_grid_search_around_best(\n",
    "    svc_search,\n",
    "    feature_list,\n",
    "    X_train, y_train,\n",
    "    pipeline_preprocess,\n",
    "    n_splits=5\n",
    "):\n",
    "    # -----------------------------\n",
    "    # 1. Extract best params\n",
    "    # -----------------------------\n",
    "    best_params = svc_search.best_params_\n",
    "    best_C = best_params[\"model__C\"]\n",
    "    best_gamma = best_params[\"model__gamma\"]\n",
    "    print(\"Random search best C:\", best_C)\n",
    "    print(\"Random search best gamma:\", best_gamma)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2. Build local grids around them\n",
    "    #    (log-spaced neighborhood)\n",
    "    # -----------------------------\n",
    "    def around_log(value, span=1.0, num=5, vmin=1e-4, vmax=1e4):\n",
    "        \"\"\"Return log-spaced values around 'value' within +/- span in log10.\"\"\"\n",
    "        center = np.log10(value)\n",
    "        low = max(center - span, np.log10(vmin))\n",
    "        high = min(center + span, np.log10(vmax))\n",
    "        return np.logspace(low, high, num)\n",
    "\n",
    "    C_grid = around_log(best_C, span=0.7, num=5, vmin=1e-2, vmax=1e3)\n",
    "    gamma_grid = around_log(best_gamma, span=0.7, num=5, vmin=1e-4, vmax=1e1)\n",
    "\n",
    "    print(\"C grid:\", C_grid)\n",
    "    print(\"gamma grid:\", gamma_grid)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3. Rebuild SVC pipeline\n",
    "    #    (same as in random search)\n",
    "    # -----------------------------\n",
    "    ct = pipeline_preprocess.named_steps[\"preprocess\"]\n",
    "    ct = clone(ct)\n",
    "\n",
    "    base_pipe = Pipeline([\n",
    "        (\"preprocess\", PreprocessToDF(ct)),\n",
    "        (\"select\",    ColumnSelectorByName(feature_list)),\n",
    "        (\"scale\",     StandardScaler()),\n",
    "        (\"model\",     make_svc()),   # fresh SVC, params set via grid\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        \"model__C\": C_grid,\n",
    "        \"model__gamma\": gamma_grid,\n",
    "        \"model__kernel\": [\"rbf\"],\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=n_splits,\n",
    "        shuffle=True,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_pipe,\n",
    "        param_grid=param_grid,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4. Fit grid search on RAW X_train\n",
    "    # -----------------------------\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best params (GridSearchCV):\", grid.best_params_)\n",
    "    print(\"Best CV F1 (GridSearchCV):\", grid.best_score_)\n",
    "\n",
    "    best_svc_pipeline = grid.best_estimator_\n",
    "    return best_svc_pipeline, grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc_grid_pipeline, svc_grid = tune_svc_grid_search_around_best(\n",
    "    svc_search=svc_search,\n",
    "    feature_list=svc_features,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    pipeline_preprocess=pipeline_preprocess,\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "svc_grid_results = evaluate_model_preprocessed(\n",
    "    model=best_svc_grid_pipeline.named_steps[\"model\"],\n",
    "    feature_list=svc_features,\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    X_test=X_test,   y_test=y_test,\n",
    "    pipeline_preprocess=pipeline_preprocess\n",
    ")\n",
    "\n",
    "svc_grid_results[\"model\"] = \"SVC_tuned_grid\"\n",
    "svc_grid_results[\"feature_set\"] = \"strict\"\n",
    "\n",
    "pd.DataFrame([svc_grid_results])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The grid search mostly confirmed the random-search best point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best model from the grid search as final SVC pipeline\n",
    "best_svc_pipeline = svc_grid.best_estimator_\n",
    "\n",
    "# (Alternatively, if you prefer the random-search one, swap to:)\n",
    "# best_svc_pipeline = svc_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pipeline already includes preprocess + select + scale + SVC\n",
    "y_test_proba = best_svc_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", alpha=0.6)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – Tuned SVC (strict features)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "ap = average_precision_score(y_test, y_test_proba)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(rec, prec, label=f\"AP = {ap:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall Curve – Tuned SVC (strict features)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "thresholds = np.linspace(0, 1, 201)\n",
    "f1_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred_t = (y_test_proba >= t).astype(int)\n",
    "    f1_scores.append(f1_score(y_test, y_pred_t))\n",
    "    precisions.append(precision_score(y_test, y_pred_t, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred_t))\n",
    "\n",
    "f1_scores = np.array(f1_scores)\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_t = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "# F1 vs threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, f1_scores, label=\"F1\", linewidth=2)\n",
    "plt.axvline(0.5, color=\"red\", linestyle=\"--\", label=\"Threshold 0.5\")\n",
    "plt.axvline(best_t, color=\"green\", linestyle=\"--\",\n",
    "            label=f\"Best on test: t={best_t:.3f}, F1={best_f1:.3f}\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"F1 vs Threshold – Tuned SVC (strict features)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Precision & Recall vs threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(thresholds, precisions, label=\"Precision\")\n",
    "plt.plot(thresholds, recalls, label=\"Recall\")\n",
    "plt.axvline(0.5, color=\"red\", linestyle=\"--\", label=\"Threshold 0.5\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision & Recall vs Threshold – Tuned SVC (strict features)\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # SVC Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold_cv(\n",
    "    pipeline,\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    n_splits=5,\n",
    "    thresholds=np.linspace(0, 1, 101)\n",
    "):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    fold_thresholds = []\n",
    "\n",
    "    cv_acc = []\n",
    "    cv_prec = []\n",
    "    cv_rec = []\n",
    "    cv_f1 = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Fit pipeline for this fold\n",
    "        pipeline.fit(X_tr, y_tr)\n",
    "\n",
    "        # Predict probabilities\n",
    "        val_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        # Search best threshold\n",
    "        best_f1 = -1\n",
    "        best_t = 0.5\n",
    "\n",
    "        for t in thresholds:\n",
    "            y_pred_t = (val_proba >= t).astype(int)\n",
    "            f1 = f1_score(y_val, y_pred_t)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_t = t\n",
    "\n",
    "        fold_thresholds.append(best_t)\n",
    "\n",
    "        # CV metrics at that threshold\n",
    "        y_pred_best = (val_proba >= best_t).astype(int)\n",
    "        cv_acc.append(accuracy_score(y_val, y_pred_best))\n",
    "        cv_prec.append(precision_score(y_val, y_pred_best, zero_division=0))\n",
    "        cv_rec.append(recall_score(y_val, y_pred_best))\n",
    "        cv_f1.append(f1_score(y_val, y_pred_best))\n",
    "\n",
    "        print(f\"Fold {fold}: best_t = {best_t:.3f}, F1 = {best_f1:.3f}\")\n",
    "\n",
    "    # Global threshold = mean of fold thresholds\n",
    "    global_t = np.mean(fold_thresholds)\n",
    "\n",
    "    # Refit pipeline on full training set\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test\n",
    "    test_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = (test_proba >= global_t).astype(int)\n",
    "\n",
    "    results = {\n",
    "        \"cv_accuracy\":  np.mean(cv_acc),\n",
    "        \"cv_precision\": np.mean(cv_prec),\n",
    "        \"cv_recall\":    np.mean(cv_rec),\n",
    "        \"cv_f1\":        np.mean(cv_f1),\n",
    "\n",
    "        \"best_threshold\": global_t,\n",
    "\n",
    "        \"test_accuracy\":  accuracy_score(y_test, y_test_pred),\n",
    "        \"test_precision\": precision_score(y_test, y_test_pred, zero_division=0),\n",
    "        \"test_recall\":    recall_score(y_test, y_test_pred),\n",
    "        \"test_f1\":        f1_score(y_test, y_test_pred),\n",
    "\n",
    "        \"per_fold_thresholds\": fold_thresholds,\n",
    "        \"global_threshold\": global_t\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_threshold_results = optimize_threshold_cv(\n",
    "    pipeline=best_svc_pipeline,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    n_splits=5\n",
    ")\n",
    "\n",
    "svc_threshold_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The previous SVC tuned model had test F1 = 0.47; after threshold tuning it improved to 0.51."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Threshold tuning recovered generalization lost during hyperparameter search. The tuned SVC now generalizes as well as XGBoost on test, while being simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Get optimized threshold\n",
    "# -----------------------------\n",
    "t_opt = float(svc_threshold_results[\"global_threshold\"])\n",
    "print(\"Using optimized threshold:\", t_opt)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Predict on test set\n",
    "# -----------------------------\n",
    "test_proba_svc = best_svc_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred_opt = (test_proba_svc >= t_opt).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Confusion matrix\n",
    "# -----------------------------\n",
    "cm = confusion_matrix(y_test, y_pred_opt)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Heatmap\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    cbar=False,\n",
    "    xticklabels=[\"Predicted: No Attrition\", \"Predicted: Attrition\"],\n",
    "    yticklabels=[\"Actual: No Attrition\", \"Actual: Attrition\"]\n",
    ")\n",
    "plt.title(f\"Tuned SVC Confusion Matrix (Threshold = {t_opt:.3f})\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Classification report\n",
    "# -----------------------------\n",
    "print(\"\\nClassification Report (Optimized Threshold):\\n\")\n",
    "print(classification_report(y_test, y_pred_opt, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1. Use optimized threshold\n",
    "# ------------------------------------------\n",
    "t_opt = float(svc_threshold_results[\"global_threshold\"])\n",
    "print(\"Using optimized threshold:\", t_opt)\n",
    "\n",
    "test_proba_svc = best_svc_pipeline.predict_proba(X_test)[:, 1]\n",
    "y_pred_opt = (test_proba_svc >= t_opt).astype(int)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Confusion matrix (raw + normalized)\n",
    "# ------------------------------------------\n",
    "cm = confusion_matrix(y_test, y_pred_opt)\n",
    "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Additional metrics\n",
    "precision = precision_score(y_test, y_pred_opt, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_opt)\n",
    "f1 = f1_score(y_test, y_pred_opt)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Plotting\n",
    "# ------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "titles = [\"Confusion Matrix (Counts)\", \"Confusion Matrix (Normalized %)\"]\n",
    "mats = [cm, cm_norm]\n",
    "fmts = [\"d\", \".2f\"]\n",
    "\n",
    "for ax, mat, title, fmt in zip(axes, mats, titles, fmts):\n",
    "\n",
    "    sns.heatmap(\n",
    "        mat,\n",
    "        annot=True,\n",
    "        fmt=fmt,\n",
    "        cmap=\"Blues\",\n",
    "        cbar=False,\n",
    "        linewidths=1,\n",
    "        linecolor=\"gray\",\n",
    "        xticklabels=[\"Predicted: No Attrition\", \"Predicted: Attrition\"],\n",
    "        yticklabels=[\"Actual: No Attrition\", \"Actual: Attrition\"],\n",
    "        ax=ax,\n",
    "        annot_kws={\"size\": 14}\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=16, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Predicted Label\", fontsize=14)\n",
    "    ax.set_ylabel(\"True Label\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. Below-plot metrics annotation\n",
    "# ------------------------------------------\n",
    "plt.figtext(\n",
    "    0.5, -0.05,\n",
    "    f\"Threshold = {t_opt:.3f} | \"\n",
    "    f\"Precision = {precision:.3f} | \"\n",
    "    f\"Recall = {recall:.3f} | \"\n",
    "    f\"Specificity = {specificity:.3f} | \"\n",
    "    f\"F1 Score = {f1:.3f}\",\n",
    "    wrap=True,\n",
    "    horizontalalignment='center',\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ------------------------------------------\n",
    "# 5. Detailed classification report in text\n",
    "# ------------------------------------------\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred_opt, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1) Grab baseline XGB row from your results_df\n",
    "xgb_base_mod = results_df.query(\"model == 'XGB' and feature_set == 'moderate'\").iloc[0]\n",
    "\n",
    "# 2) Build comparison table (test metrics only)\n",
    "rows = [\n",
    "    {\n",
    "        \"model\": \"XGB_baseline (moderate)\",\n",
    "        \"Accuracy\":  xgb_base_mod[\"test_accuracy\"],\n",
    "        \"Precision\": xgb_base_mod[\"test_precision\"],\n",
    "        \"Recall\":    xgb_base_mod[\"test_recall\"],\n",
    "        \"F1\":        xgb_base_mod[\"test_f1\"],\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"SVC_tuned + thresh (strict)\",\n",
    "        \"Accuracy\":  float(svc_threshold_results[\"test_accuracy\"]),\n",
    "        \"Precision\": float(svc_threshold_results[\"test_precision\"]),\n",
    "        \"Recall\":    float(svc_threshold_results[\"test_recall\"]),\n",
    "        \"F1\":        float(svc_threshold_results[\"test_f1\"]),\n",
    "    }\n",
    "]\n",
    "\n",
    "df_compare = pd.DataFrame(rows)\n",
    "display(df_compare)\n",
    "\n",
    "# 3) Melt to long format for plotting\n",
    "df_long = df_compare.melt(id_vars=\"model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "metric_order = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "df_long[\"Metric\"] = pd.Categorical(df_long[\"Metric\"], categories=metric_order, ordered=True)\n",
    "\n",
    "# 4) Side-by-side bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    data=df_long,\n",
    "    x=\"Metric\",\n",
    "    y=\"Score\",\n",
    "    hue=\"model\"\n",
    ")\n",
    "\n",
    "plt.ylim(0, 1.0)\n",
    "plt.title(\"Model Comparison – XGB Baseline vs Tuned SVC\", fontsize=14, fontweight=\"bold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(\"\")\n",
    "plt.legend(title=\"Model\")\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SVC params\n",
    "svc_model = best_svc_pipeline.named_steps[\"model\"]\n",
    "svc_params = {\n",
    "    \"C\": svc_model.C,\n",
    "    \"gamma\": svc_model.gamma,\n",
    "    \"kernel\": svc_model.kernel,\n",
    "    \"class_weight\": svc_model.class_weight,\n",
    "    \"probability\": svc_model.probability,\n",
    "}\n",
    "\n",
    "xgb_card = {\n",
    "    \"model_name\": \"XGB_baseline\",\n",
    "    \"feature_set\": \"moderate\",\n",
    "    \"algorithm\": \"XGBoost (tree-based gradient boosting)\",\n",
    "    \"threshold\": 0.50,  # or xgb_threshold_results['global_threshold'] if you optimized\n",
    "    \"cv_accuracy\":  xgb_base_mod[\"cv_accuracy\"],\n",
    "    \"cv_precision\": xgb_base_mod[\"cv_precision\"],\n",
    "    \"cv_recall\":    xgb_base_mod[\"cv_recall\"],\n",
    "    \"cv_f1\":        xgb_base_mod[\"cv_f1\"],\n",
    "    \"test_accuracy\":  xgb_base_mod[\"test_accuracy\"],\n",
    "    \"test_precision\": xgb_base_mod[\"test_precision\"],\n",
    "    \"test_recall\":    xgb_base_mod[\"test_recall\"],\n",
    "    \"test_f1\":        xgb_base_mod[\"test_f1\"],\n",
    "}\n",
    "\n",
    "svc_card = {\n",
    "    \"model_name\": \"SVC_tuned_thresholded\",\n",
    "    \"feature_set\": \"strict\",\n",
    "    \"algorithm\": \"Support Vector Classifier (RBF kernel)\",\n",
    "    \"threshold\": float(svc_threshold_results[\"global_threshold\"]),\n",
    "    \"svc_params\": svc_params,\n",
    "    \"cv_accuracy\":  float(svc_threshold_results[\"cv_accuracy\"]),\n",
    "    \"cv_precision\": float(svc_threshold_results[\"cv_precision\"]),\n",
    "    \"cv_recall\":    float(svc_threshold_results[\"cv_recall\"]),\n",
    "    \"cv_f1\":        float(svc_threshold_results[\"cv_f1\"]),\n",
    "    \"test_accuracy\":  float(svc_threshold_results[\"test_accuracy\"]),\n",
    "    \"test_precision\": float(svc_threshold_results[\"test_precision\"]),\n",
    "    \"test_recall\":    float(svc_threshold_results[\"test_recall\"]),\n",
    "    \"test_f1\":        float(svc_threshold_results[\"test_f1\"]),\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "model_cards_df = pd.DataFrame([xgb_card, svc_card])\n",
    "model_cards_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# Mapping encoded strict features back to raw feature name (reuse the same logic we used before)\n",
    "\n",
    "def get_raw_feature_name(encoded_feature):\n",
    "    \"\"\"\n",
    "    'ord__JobLevel'                         -> 'JobLevel'\n",
    "    'num__Age'                              -> 'Age'\n",
    "    'nom__OverTime_1'                       -> 'OverTime'\n",
    "    'nom__BusinessTravel_Travel_Rarely'     -> 'BusinessTravel'\n",
    "    \"\"\"\n",
    "    if encoded_feature.startswith((\"num__\", \"ord__\")):\n",
    "        return encoded_feature.split(\"__\", 1)[1]\n",
    "    if encoded_feature.startswith(\"nom__\"):\n",
    "        tmp = encoded_feature.split(\"__\", 1)[1]  # 'Column_Category...'\n",
    "        raw_col = tmp.split(\"_\", 1)[0]\n",
    "        return raw_col\n",
    "    return encoded_feature\n",
    "\n",
    "# strict encoded feature list you used for SVC\n",
    "svc_features_encoded = features_strict\n",
    "\n",
    "strict_raw_features = sorted(\n",
    "    {get_raw_feature_name(f) for f in svc_features_encoded}\n",
    ")\n",
    "\n",
    "print(\"Raw features used by SVC (strict set):\")\n",
    "print(strict_raw_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction function over *raw* X (full X, pipeline handles preprocessing)\n",
    "\n",
    "# We'll pass full X (all columns) to the pipeline.\n",
    "all_feature_names = list(X_train.columns)\n",
    "\n",
    "def f_pred(X_array):\n",
    "    \"\"\"\n",
    "    X_array: numpy array with columns in the same order as X_train.columns\n",
    "    Returns: P(Attrition=1)\n",
    "    \"\"\"\n",
    "    X_df = pd.DataFrame(X_array, columns=all_feature_names)\n",
    "    return best_svc_pipeline.predict_proba(X_df)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Background data for KernelExplainer\n",
    "\n",
    "background_size = 100\n",
    "background_df = X_train.sample(\n",
    "    n=min(background_size, len(X_train)),\n",
    "    random_state=42\n",
    ")\n",
    "background = background_df.values\n",
    "\n",
    "\n",
    "# Data to explain (subset of test set)\n",
    "\n",
    "n_explain = 200\n",
    "X_test_explain_df = X_test.sample(\n",
    "    n=min(n_explain, len(X_test)),\n",
    "    random_state=123\n",
    ")\n",
    "X_test_explain = X_test_explain_df.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Run KernelExplainer\n",
    "# -------------------------------------------------\n",
    "explainer = shap.KernelExplainer(f_pred, background)\n",
    "shap_values_full = explainer.shap_values(X_test_explain, nsamples=\"auto\")\n",
    "\n",
    "# shap_values_full can be list or array depending on SHAP version\n",
    "if isinstance(shap_values_full, list):\n",
    "    shap_values_full = shap_values_full[0]  # for scalar output\n",
    "\n",
    "shap_values_full = np.array(shap_values_full)  # (n_samples, n_all_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Restrict SHAP & X to the strict raw features\n",
    "# -------------------------------------------------\n",
    "# indices of strict_raw_features in the full DataFrame\n",
    "idx_strict = [all_feature_names.index(col) for col in strict_raw_features]\n",
    "\n",
    "shap_values_strict = shap_values_full[:, idx_strict]\n",
    "X_explain_strict = X_test_explain_df[strict_raw_features]\n",
    "\n",
    "print(\"SHAP array shape (strict):\", shap_values_strict.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SHAP plots (lay-audience friendly feature names)\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Bar plot: global importance\n",
    "shap.summary_plot(\n",
    "    shap_values_strict,\n",
    "    X_explain_strict,\n",
    "    feature_names=strict_raw_features,\n",
    "    plot_type=\"bar\",\n",
    "    max_display=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beeswarm: direction + distribution\n",
    "shap.summary_plot(\n",
    "    shap_values_strict,\n",
    "    X_explain_strict,\n",
    "    feature_names=strict_raw_features,\n",
    "    max_display=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# SHAP values -> DataFrame for convenience\n",
    "df_shap = pd.DataFrame(shap_values_strict, columns=strict_raw_features)\n",
    "df_feats = X_explain_strict[strict_raw_features].copy()\n",
    "\n",
    "# Simple dtype-based split\n",
    "numeric_features = [c for c in strict_raw_features if df_feats[c].dtype != \"object\"]\n",
    "categorical_features = [c for c in strict_raw_features if df_feats[c].dtype == \"object\"]\n",
    "\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "print(\"Categorical features:\", categorical_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_cats = [\"OverTime\", \"JobRole\", \"MaritalStatus\", \"BusinessTravel\"]\n",
    "for c in force_cats:\n",
    "    if c in strict_raw_features and c not in categorical_features:\n",
    "        categorical_features.append(c)\n",
    "        if c in numeric_features:\n",
    "            numeric_features.remove(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "if numeric_features:\n",
    "    shap.summary_plot(\n",
    "        df_shap[numeric_features].values,\n",
    "        df_feats[numeric_features],\n",
    "        feature_names=numeric_features,\n",
    "        max_display=20\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: nicer labels for OverTime, etc.\n",
    "overtime_labels = {0: \"No overtime\", 1: \"Overtime\"}\n",
    "\n",
    "for feat in categorical_features:\n",
    "    shap_vals = df_shap[feat]\n",
    "    vals = df_feats[feat].copy()\n",
    "\n",
    "    # Apply human-friendly mapping where useful\n",
    "    if feat == \"OverTime\":\n",
    "        vals = vals.map(overtime_labels).fillna(vals)\n",
    "\n",
    "    # Build plotting DataFrame\n",
    "    plot_df = pd.DataFrame({\n",
    "        \"SHAP value\": shap_vals,\n",
    "        feat: vals\n",
    "    })\n",
    "\n",
    "    # Create a palette with one colour per category\n",
    "    categories = plot_df[feat].astype(str).unique()\n",
    "    palette = sns.color_palette(\"Set2\", len(categories))\n",
    "\n",
    "    plt.figure(figsize=(8, max(3, 0.5 * len(categories))))\n",
    "    sns.stripplot(\n",
    "        data=plot_df,\n",
    "        x=\"SHAP value\",\n",
    "        y=feat,\n",
    "        hue=feat,\n",
    "        palette=palette,\n",
    "        dodge=False,\n",
    "        alpha=0.7,\n",
    "        orient=\"h\",\n",
    "        jitter=0.25,\n",
    "        linewidth=0.5,\n",
    "        edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "    plt.axvline(0, color=\"gray\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.title(f\"SHAP values for {feat}\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Impact on attrition prediction (SHAP value)\")\n",
    "    plt.ylabel(feat)\n",
    "    plt.legend(title=feat, bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
